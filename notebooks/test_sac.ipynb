{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa780004",
   "metadata": {},
   "source": [
    "# SAC smoke test on HalfCheetah-v5\n",
    "\n",
    "Minimal setup that instantiates the SAC agent with a short HalfCheetah-v5 rollout so we can sanity-check wiring before running long experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53543d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "from tensordict.nn import InteractionType, TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn, optim\n",
    "from torchrl.collectors import aSyncDataCollector, SyncDataCollector\n",
    "from torchrl.data import (\n",
    "    LazyMemmapStorage,\n",
    "    LazyTensorStorage,\n",
    "    TensorDictPrioritizedReplayBuffer,\n",
    "    TensorDictReplayBuffer,\n",
    ")\n",
    "from torchrl.envs import (\n",
    "    CatTensors,\n",
    "    Compose,\n",
    "    DMControlEnv,\n",
    "    DoubleToFloat,\n",
    "    EnvCreator,\n",
    "    ParallelEnv,\n",
    "    TransformedEnv,\n",
    ")\n",
    "from torchrl.envs.libs.gym import GymEnv, set_gym_backend\n",
    "from torchrl.envs.transforms import InitTracker, RewardSum, StepCounter\n",
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "from torchrl.modules import MLP, ProbabilisticActor, ValueOperator\n",
    "from torchrl.modules.distributions import TanhNormal\n",
    "from torchrl.objectives import SoftUpdate\n",
    "from torchrl.objectives.sac import SACLoss\n",
    "from torchrl.record import VideoRecorder\n",
    "\n",
    "# ====================================================================\n",
    "# Environment utils\n",
    "# -----------------\n",
    "\n",
    "\n",
    "def env_maker(cfg, device=\"cpu\", from_pixels=False):\n",
    "    lib = cfg.env.library\n",
    "    if lib in (\"gym\", \"gymnasium\"):\n",
    "        with set_gym_backend(lib):\n",
    "            return GymEnv(\n",
    "                cfg.env.name,\n",
    "                device=device,\n",
    "                from_pixels=from_pixels,\n",
    "                pixels_only=False,\n",
    "            )\n",
    "    elif lib == \"dm_control\":\n",
    "        env = DMControlEnv(\n",
    "            cfg.env.name, cfg.env.task, from_pixels=from_pixels, pixels_only=False\n",
    "        )\n",
    "        return TransformedEnv(\n",
    "            env, CatTensors(in_keys=env.observation_spec.keys(), out_key=\"observation\")\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unknown lib {lib}.\")\n",
    "\n",
    "\n",
    "def apply_env_transforms(env, max_episode_steps=1000):\n",
    "    transformed_env = TransformedEnv(\n",
    "        env,\n",
    "        Compose(\n",
    "            InitTracker(),\n",
    "            StepCounter(max_episode_steps),\n",
    "            DoubleToFloat(),\n",
    "            RewardSum(),\n",
    "        ),\n",
    "    )\n",
    "    return transformed_env\n",
    "\n",
    "\n",
    "def make_environment(cfg, logger=None):\n",
    "    \"\"\"Make environments for training and evaluation.\"\"\"\n",
    "    partial = functools.partial(env_maker, cfg=cfg)\n",
    "    parallel_env = ParallelEnv(\n",
    "        cfg.collector.env_per_collector,\n",
    "        EnvCreator(partial),\n",
    "        serial_for_single=True,\n",
    "    )\n",
    "    parallel_env.set_seed(cfg.env.seed)\n",
    "\n",
    "    train_env = apply_env_transforms(parallel_env, cfg.env.max_episode_steps)\n",
    "\n",
    "    partial = functools.partial(env_maker, cfg=cfg, from_pixels=cfg.logger.video)\n",
    "    trsf_clone = train_env.transform.clone()\n",
    "    if cfg.logger.video:\n",
    "        trsf_clone.insert(\n",
    "            0, VideoRecorder(logger, tag=\"rendering/test\", in_keys=[\"pixels\"])\n",
    "        )\n",
    "    eval_env = TransformedEnv(\n",
    "        ParallelEnv(\n",
    "            cfg.collector.env_per_collector,\n",
    "            EnvCreator(partial),\n",
    "            serial_for_single=True,\n",
    "        ),\n",
    "        trsf_clone,\n",
    "    )\n",
    "    return train_env, eval_env\n",
    "\n",
    "\n",
    "def make_train_environment(cfg):\n",
    "    \"\"\"Make environments for training and evaluation.\"\"\"\n",
    "    partial = functools.partial(env_maker, cfg=cfg)\n",
    "    parallel_env = ParallelEnv(\n",
    "        cfg.collector.env_per_collector,\n",
    "        EnvCreator(partial),\n",
    "        serial_for_single=True,\n",
    "    )\n",
    "    parallel_env.set_seed(cfg.env.seed)\n",
    "\n",
    "    train_env = apply_env_transforms(parallel_env, cfg.env.max_episode_steps)\n",
    "\n",
    "    return train_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc22a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from rlopt.agent.sac import SAC, SACRLOptConfig\n",
    "from rlopt.config_base import NetworkConfig\n",
    "from rlopt.env_utils import env_maker\n",
    "\n",
    "\n",
    "def build_halfcheetah_config(total_frames: int = 1024) -> SACRLOptConfig:\n",
    "    \"\"\"Return a minimally tuned SAC config for HalfCheetah-v5 smoke tests.\"\"\"\n",
    "\n",
    "    cfg = SACRLOptConfig()\n",
    "    cfg.seed = 7\n",
    "    cfg.device = \"auto\"\n",
    "\n",
    "    # Environment + collector knobs -----------------------------------------\n",
    "    cfg.env.env_name = \"HalfCheetah-v5\"\n",
    "    cfg.env.library = \"gymnasium\"\n",
    "    cfg.env.num_envs = 8\n",
    "\n",
    "    cfg.collector.frames_per_batch = 1000\n",
    "    cfg.collector.total_frames = total_frames\n",
    "    cfg.collector.init_random_frames = 25_000\n",
    "    cfg.collector.prefetch = 1\n",
    "\n",
    "    # Replay + optimization -------------------------------------------------\n",
    "    cfg.loss.mini_batch_size = 256\n",
    "    cfg.replay_buffer.size = 1_000_000\n",
    "    cfg.replay_buffer.prefetch = 1\n",
    "    cfg.optim.lr = 3e-4\n",
    "    cfg.optim.scheduler = None\n",
    "    cfg.optim.target_update_polyak = 0.995\n",
    "    cfg.sac.utd_ratio = 1.0\n",
    "\n",
    "    # Lightweight logging so the notebook runs without external services ----\n",
    "    log_dir = Path.cwd() / \"notebook_logs\"\n",
    "    cfg.logger.backend = \"\"\n",
    "    cfg.logger.log_to_file = True\n",
    "    cfg.logger.log_dir = str(log_dir)\n",
    "    cfg.logger.exp_name = \"sac_halfcheetah_smoketest\"\n",
    "    cfg.logger.python_level = \"info\"\n",
    "\n",
    "    # Network dimensions depend on env specs --------------------------------\n",
    "    dummy_env = gym.make(cfg.env.env_name)\n",
    "    obs_dim = dummy_env.observation_space.shape[0]\n",
    "    action_dim = dummy_env.action_space.shape[0]\n",
    "    dummy_env.close()\n",
    "\n",
    "    cfg.policy.input_dim = obs_dim\n",
    "    cfg.policy.num_cells = [256, 256]\n",
    "    cfg.policy.activation_fn = \"relu\"\n",
    "\n",
    "    cfg.q_function = NetworkConfig(\n",
    "        num_cells=[256, 256],\n",
    "        input_dim=obs_dim + action_dim,\n",
    "        input_keys=[\"observation\", \"action\"],\n",
    "        activation_fn=\"relu\",\n",
    "    )\n",
    "    cfg.collector.env_per_collector = cfg.env.num_envs\n",
    "    cfg.env.seed = 8\n",
    "    cfg.env.max_episode_steps = 1000\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "cfg = build_halfcheetah_config(total_frames=1_000_000)\n",
    "train_env, eval_env = make_environment(cfg, logger=None)\n",
    "agent = SAC(env=train_env, eval_env=eval_env, config=cfg)\n",
    "agent.train()\n",
    "\n",
    "# Quick deterministic rollout on a fresh eval env to verify predict() -------\n",
    "eval_env = env_maker(cfg, device=cfg.device)\n",
    "with torch.no_grad():\n",
    "    td = eval_env.reset().to(agent.device)\n",
    "    action = agent.predict(td.clone())\n",
    "print(f\"Deterministic action sample (shape={tuple(action.shape)}):\\n{action}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
